{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as un\n",
    "from astroquery.simbad import Simbad\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate flare star catalogues\n",
    "\n",
    "Here we read and collate catalogues of flare type objects. The catalogues we use include flare stars, white dwarfs, RS CVns, unidentified transient flares and more. The catalogues we use here are:\n",
    "\n",
    "- Lepine: bright M-dwarfs - https://ui.adsabs.harvard.edu/abs/2011AJ....142..138L/abstract\n",
    "- JGagne: ultra-cool dwarfs - jgagneastro.wordpress.com/list-of-ultracool-dwarfs\n",
    "- sdss_mwds: magnetic white dwarfs from SDSS: https://ui.adsabs.harvard.edu/abs/2013MNRAS.429.2934K/abstract\n",
    "- Catalina: Transients in the Catalina Surveys Data Release 2 - https://ui.adsabs.harvard.edu/abs/2009ApJ...696..870D/abstract\n",
    "- simbad_fstars: flare stars (F*) from the SIMBAD catalogue (2000A&AS..143....9W)\n",
    "- simbad_RSCVns: flare stars (RS*) from the SIMBAD catalogue (2000A&AS..143....9W)\n",
    "- whitedwarfs: white dwarfs from the Montreal White Dwarf Database - http://www.montrealwhitedwarfdatabase.org/references.html\n",
    "\n",
    "We read in each catalogue using pandas, make columns for the proper motion if there are none and rename the RA and DEC columns. Then we combine the catalogues. If the source does not have an official SIMBAD name already, we check SIMBAD by radius to see if the source is known. Please note that this means that the SIMBAD names are *not* confirmed, but are a good starting point for looking into the source. Also, not all sources are identified in SIMBAD, particularly Catalina transients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files for the catalogues\n",
    "\n",
    "# Path to catalogue files\n",
    "path = '/raid/driessen/Catalogues/'\n",
    "\n",
    "# The catalogue files, downloaded from the\n",
    "# above sources\n",
    "lepine = ('{}Lepine_BrightMDwarfs.csv'.format(path))\n",
    "jgagne = ('{}List_of_UltraCool_Dwarfs.csv'.format(path))\n",
    "sdss_mwds = ('{}magWDs.tsv'.format(path))\n",
    "simbad_fstars = ('{}simbad_flarestars_pm.txt'.format(path))\n",
    "catalina = ('{}CRTS_all_transients.tsv'.format(path))\n",
    "catalinas = glob.glob('{}css_transientcandidates_?.tsv'.format(path))\n",
    "whitedwarfs = ('{}MWDD-export.csv'.format(path))\n",
    "simbad_RSCVns = ('{}SIMBAD_RSCVns.txt'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files\n",
    "\n",
    "Read in the files usin pandas. Each one has to be read in slightly differently because the catalogues are formatted very differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lepine_source_info = []\n",
    "columns = ['lepine_name', 'cns3_name',\n",
    "           'ra(deg)', 'dec(deg)',\n",
    "           'pmra(arcsec/yr)', 'pmdec(arcsec/yr)']\n",
    "\n",
    "# Lepine uses an odd file format that Pandas\n",
    "# doesn't like, so read it in line by line\n",
    "with open(lepine, 'r') as fn:\n",
    "    for l, line in enumerate(fn):\n",
    "        if l > 40:\n",
    "            lepinename = line[:17].strip()\n",
    "            cnsname = line[38:55].strip()\n",
    "            ra = line[56:66].strip()\n",
    "            dec = line[68:78].strip()\n",
    "            pmra = line[86:92].strip()\n",
    "            pmdec = line[93:99].strip()\n",
    "            \n",
    "            lepine_source_info.append([lepinename, cnsname,\n",
    "                                       ra, dec, pmra, pmdec])  \n",
    "lepine_source_info = np.array(lepine_source_info)\n",
    "\n",
    "# Put everything into a Pandas table\n",
    "lepine_dict = dict()\n",
    "for c, col in enumerate(columns):\n",
    "    lepine_dict[col] = lepine_source_info[:, c]\n",
    "lepine_table = pd.DataFrame(data=lepine_dict)\n",
    "\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "lepine_table = lepine_table.dropna(axis='rows', subset=['ra(deg)',\n",
    "                                                        'dec(deg)'])\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "lepine_table['Catalogue'] = 'Lepine'\n",
    "# Add an empty column for SIMBAD names\n",
    "lepine_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jgagne_table = pd.read_csv(jgagne)\n",
    "# Change the column names so that every\n",
    "# catalogue has the same name\n",
    "# for the RA and Dec columns\n",
    "jgagne_table = jgagne_table.rename(columns={'R.A. (deg)':'ra(deg)',\n",
    "                                            'Decl. (deg)':'dec(deg)'})\n",
    "jgagne_table = jgagne_table.dropna(axis='rows', subset=['ra(deg)',\n",
    "                                                        'dec(deg)'])\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "jgagne_table['Catalogue'] = 'J.Gagne'\n",
    "# Add an empty column for SIMBAD names\n",
    "# and proper motions\n",
    "jgagne_table['pmra(arcsec/yr)'] = ''\n",
    "jgagne_table['pmdec(arcsec/yr)'] = ''\n",
    "jgagne_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizier_mwds_table = pd.read_csv(sdss_mwds, header=[71, 72], delimiter='\\t')\n",
    "vizier_mwds_table = vizier_mwds_table[1:]\n",
    "vizier_mwds_table.columns = vizier_mwds_table.columns.map('_'.join)\n",
    "\n",
    "# Change the column names so that every\n",
    "# catalogue has the same name\n",
    "# for the RA and Dec columns\n",
    "vizier_mwds_table = vizier_mwds_table.rename(columns={'#_RAJ2000_#deg':'ra(deg)',\n",
    "                                                      '_DEJ2000_deg':'dec(deg)'})\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "vizier_mwds_table = vizier_mwds_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "vizier_mwds_table['Catalogue'] = 'Vizier_MWDs'\n",
    "# Add an empty column for SIMBAD names\n",
    "# and proper motions\n",
    "vizier_mwds_table['pmra(arcsec/yr)'] = ''\n",
    "vizier_mwds_table['pmdec(arcsec/yr)'] = ''\n",
    "vizier_mwds_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbad_fstars_table = pd.read_csv(simbad_fstars, header=[4], delimiter='|')\n",
    "simbad_fstars_table = simbad_fstars_table[1:-1]\n",
    "simbad_fstars_table.columns = simbad_fstars_table.columns.str.strip()\n",
    "\n",
    "# This catalogue has an odd coordinate\n",
    "# format, so I first correct that\n",
    "simbad_coords = []\n",
    "simbad_coords_orig = simbad_fstars_table['coord1 (ICRS,J2000/2000)']\n",
    "\n",
    "for c, coo in enumerate(simbad_coords_orig):\n",
    "    if '+' in coo:\n",
    "        coordi = coo.split(' +')\n",
    "        coordi = [coordi[0], '+'+coordi[1]]\n",
    "    elif '-' in coo:\n",
    "        coordi = coo.split(' -')\n",
    "        coordi = [coordi[0], '-'+coordi[1]]\n",
    "    simbad_coords.append(coordi)\n",
    "simbad_coords = coord.SkyCoord(simbad_coords, unit=(un.hourangle, un.deg))\n",
    "# Add the corrected RA and DEC columns\n",
    "simbad_fstars_table['ra(deg)'] = np.array(simbad_coords.ra.deg)\n",
    "simbad_fstars_table['dec(deg)'] = np.array(simbad_coords.dec.deg)\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "simbad_fstars_table = simbad_fstars_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "\n",
    "# Get the proper motions\n",
    "# and put them into the correct\n",
    "# format\n",
    "pmra = []\n",
    "pmdec = []\n",
    "for pm in simbad_fstars_table['pm']:\n",
    "    if '~' in pm:\n",
    "        pmra.append(np.nan)\n",
    "        pmdec.append(np.nan)\n",
    "    else:\n",
    "        pms = pm.split()\n",
    "        pmra.append(pms[0])\n",
    "        pmdec.append(pms[1])\n",
    "simbad_fstars_table['pmra(arcsec/yr)'] = pmra\n",
    "simbad_fstars_table['pmdec(arcsec/yr)'] = pmdec\n",
    "\n",
    "# Change the column name that has\n",
    "# the SIMBAD names in it, for\n",
    "# consistancy\n",
    "simbad_fstars_table = simbad_fstars_table.rename(columns={'identifier':\n",
    "                                                          'simbad_names'})\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "simbad_fstars_table['Catalogue'] = 'SimbadFlareStars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the Catalina sources is a bit more\n",
    "# complicated because the sources are in\n",
    "# a set of files, rather than one file\n",
    "\n",
    "headers = ['CRTS_ID', 'ra(deg)', 'dec(deg)', 'classification']\n",
    "new_content = []\n",
    "with open(catalina, 'r') as content:\n",
    "    for l, line in enumerate(content):\n",
    "        if l>0:\n",
    "            line = line.split('\\t')\n",
    "            if len(line) >= 10:\n",
    "                nc = [line[0].strip(), line[1].strip(), line[2].strip(), line[-1].strip()]\n",
    "                new_content.append(nc)\n",
    "values = np.array(new_content, dtype=str)\n",
    "\n",
    "catalina_dict = dict()\n",
    "\n",
    "for h, head in enumerate(headers):\n",
    "    catalina_dict[head] = values[:, h]\n",
    "catalina_table = pd.DataFrame(data=catalina_dict)\n",
    "\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "catalina_table = catalina_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "\n",
    "for cat in catalinas:\n",
    "    headers = ['CRTS_ID', 'ra(deg)', 'dec(deg)', 'classification']\n",
    "    new_content = []\n",
    "    with open(cat, 'r') as content:\n",
    "        for l, line in enumerate(content):\n",
    "            if l>0:\n",
    "                line = line.split('\\t')\n",
    "                if len(line) >= 10:\n",
    "                    nc = [line[0].strip(), line[1].strip(), line[2].strip(), line[-1].strip()]\n",
    "                    new_content.append(nc)\n",
    "    values = np.array(new_content, dtype=str)\n",
    "\n",
    "    cat_dict = dict()\n",
    "\n",
    "    for h, head in enumerate(headers):\n",
    "        cat_dict[head] = values[:, h]\n",
    "    cat_table = pd.DataFrame(data=cat_dict)\n",
    "    \n",
    "    # Remove rows that don't have known\n",
    "    # coordinates\n",
    "    cat_table = cat_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "\n",
    "    catalina_table = catalina_table.append(cat_table)\n",
    "\n",
    "subs = ['SN', 'Ast', 'AGN', 'Nothing', 'Blazar']\n",
    "for sub in subs:\n",
    "    catalina_table['match_indexes'] = catalina_table['classification'].str.find(sub)\n",
    "    catalina_table = catalina_table[catalina_table['match_indexes'] == -1]\n",
    "    catalina_table.drop('match_indexes', axis='columns', inplace=True)\n",
    "\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "catalina_table['Catalogue'] = 'CRTS'\n",
    "# Add an empty column for SIMBAD names\n",
    "# and proper motions\n",
    "catalina_table['pmra(arcsec/yr)'] = ''\n",
    "catalina_table['pmdec(arcsec/yr)'] = ''\n",
    "catalina_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_table = pd.read_csv(whitedwarfs)\n",
    "\n",
    "# Put the coordinates in the right\n",
    "# format then add them as columns\n",
    "wd_coords = coord.SkyCoord(wd_table['icrsra'],\n",
    "                           wd_table['icrsdec'],\n",
    "                           unit=(un.hourangle, un.deg))\n",
    "wd_table['ra(deg)'] = wd_coords.ra.deg\n",
    "wd_table['dec(deg)'] = wd_coords.dec.deg\n",
    "# Change the name column for consistency\n",
    "wd_table = wd_table.rename(columns={'wdid':'simbad_names'})\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "wd_table['Catalogue'] = 'MWDD'\n",
    "# Make empty columns for the\n",
    "# proper motions\n",
    "wd_table['pmra(arcsec/yr)'] = ''\n",
    "wd_table['pmdec(arcsec/yr)'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbad_rscvns_table = pd.read_csv(simbad_RSCVns, header=[4], delimiter='|')\n",
    "simbad_rscvns_table = simbad_rscvns_table[1:-1]\n",
    "simbad_rscvns_table.columns = simbad_rscvns_table.columns.str.strip()\n",
    "\n",
    "# This catalogue has an odd coordinate\n",
    "# format, so I first correct that\n",
    "simbad_coords = []\n",
    "simbad_coords_orig = simbad_rscvns_table['coord1 (ICRS,J2000/2000)']\n",
    "\n",
    "for c, coo in enumerate(simbad_coords_orig):\n",
    "    if '+' in coo:\n",
    "        coordi = coo.split(' +')\n",
    "        coordi = [coordi[0], '+'+coordi[1]]\n",
    "    elif '-' in coo:\n",
    "        coordi = coo.split(' -')\n",
    "        coordi = [coordi[0], '-'+coordi[1]]\n",
    "    simbad_coords.append(coordi)\n",
    "simbad_coords = coord.SkyCoord(simbad_coords, unit=(un.hourangle,\n",
    "                                                    un.deg))\n",
    "# Add the corrected RA and DEC columns\n",
    "simbad_rscvns_table['ra(deg)'] = np.array(simbad_coords.ra.deg)\n",
    "simbad_rscvns_table['dec(deg)'] = np.array(simbad_coords.dec.deg)\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "simbad_rscvns_table = simbad_rscvns_table.dropna(axis='rows',\n",
    "                                                 subset=['ra(deg)',\n",
    "                                                         'dec(deg)'])\n",
    "\n",
    "# Get the proper motions\n",
    "# and put them into the correct\n",
    "# format\n",
    "pmra = []\n",
    "pmdec = []\n",
    "for pm in simbad_rscvns_table['pm']:\n",
    "    if '~' in pm:\n",
    "        pmra.append(np.nan)\n",
    "        pmdec.append(np.nan)\n",
    "    else:\n",
    "        pms = pm.split()\n",
    "        pmra.append(pms[0])\n",
    "        pmdec.append(pms[1])\n",
    "simbad_rscvns_table['pmra(arcsec/yr)'] = pmra\n",
    "simbad_rscvns_table['pmdec(arcsec/yr)'] = pmdec\n",
    "# Change the column name that has\n",
    "# the SIMBAD names in it, for\n",
    "# consistancy\n",
    "simbad_rscvns_table = simbad_rscvns_table.rename(columns={'identifier':\n",
    "                                                          'simbad_names'})\n",
    "\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "simbad_rscvns_table['Catalogue'] = 'SimbadRSCVns'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataframes\n",
    "\n",
    "Combine the pandas dataframes for each catalogue. Use \"inner\" so that only identical columns are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [lepine_table,\n",
    "              jgagne_table,\n",
    "              vizier_mwds_table,\n",
    "              simbad_fstars_table,\n",
    "              catalina_table,\n",
    "              wd_table,\n",
    "              simbad_rscvns_table]\n",
    "\n",
    "combined_df = pd.concat(dataframes, join='inner', ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "combined_df[['ra(deg)',\n",
    "             'dec(deg)',\n",
    "             'pmra(arcsec/yr)',\n",
    "             'pmdec(arcsec/yr)']] = combined_df[['ra(deg)',\n",
    "                                                 'dec(deg)',\n",
    "                                                 'pmra(arcsec/yr)',\n",
    "                                                 'pmdec(arcsec/yr)']].apply(pd.to_numeric,\n",
    "                                                                            downcast='float')\n",
    "# Split the catalogue into sources that already\n",
    "# have SIMBAD names, and sources that don't\n",
    "no_names = combined_df[combined_df['simbad_names'] == '']\n",
    "names = combined_df[combined_df['simbad_names'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find SIMBAD names for sources that don't have them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No SIMBAD matches within 2 asec:  5000 6000\n",
      "No SIMBAD matches within 2 asec:  6000 7000\n",
      "No SIMBAD matches within 2 asec:  7000 8000\n"
     ]
    }
   ],
   "source": [
    "# Get the coordinates of the sources\n",
    "# that don't have names yet\n",
    "no_name_coords = coord.SkyCoord(np.array(no_names['ra(deg)'])*un.deg,\n",
    "                                np.array(no_names['dec(deg)'])*un.deg,\n",
    "                                pm_ra_cosdec=np.array(no_names['pmra(arcsec/yr)'])*un.arcsec/un.yr,\n",
    "                                pm_dec=np.array(no_names['pmdec(arcsec/yr)'])*un.arcsec/un.yr)\n",
    "no_name_names = []\n",
    "\n",
    "# Divide the sources up into chunks,\n",
    "# otherwise astroquery will chuck\n",
    "# a hissy fit\n",
    "for s, starts in enumerate(np.arange(0, 16)):\n",
    "    start = starts * 1000\n",
    "    end = start + 1000\n",
    "    coords = no_name_coords[start:end]\n",
    "\n",
    "    # Use the coordinates to search for nearby\n",
    "    # sources for each source\n",
    "    result_table = Simbad.query_region(coords, radius=2.*un.arcsec)\n",
    "    # Match the results to the sources\n",
    "    try:\n",
    "        result_coords = coord.SkyCoord(list(result_table['RA']),\n",
    "                                       list(result_table['DEC']),\n",
    "                                       unit=(un.hourangle, un.deg))\n",
    "\n",
    "        for c, coo in enumerate(coords):\n",
    "            seps = coo.separation(result_coords)\n",
    "\n",
    "            if np.nanmin(seps.deg) < 2./60./60.:\n",
    "                no_name_names.append(result_table[np.nanargmin(seps.deg)]['MAIN_ID'].decode('UTF-8'))\n",
    "            else:\n",
    "                no_name_names.append('')\n",
    "    except (TypeError, KeyError) as e:\n",
    "        # Add a space for any source that doesn't\n",
    "        # have any SIMBAD matches\n",
    "        print('No SIMBAD matches within 2 asec: ', start, end)\n",
    "        for c, coo in enumerate(coords):\n",
    "            no_name_names.append('')\n",
    "    # Take a break, because otherwise astroquery\n",
    "    # and SIMBAD will have a different hissy fit\n",
    "    time.sleep(10)\n",
    "\n",
    "# Add the SIMBAD names you just found\n",
    "# to the source without names\n",
    "no_names['simbad_names'] = no_name_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tables again\n",
    "combined_df_names = pd.concat([names, no_names], join='inner', ignore_index=True)\n",
    "# Split them into sources with and without names again\n",
    "new_no_names = combined_df_names[combined_df_names['simbad_names'] == '']\n",
    "new_names = combined_df_names[combined_df_names['simbad_names'] != '']\n",
    "# Remove sources that have the same name\n",
    "new_names = new_names.drop_duplicates(subset='simbad_names')\n",
    "# Combine them again, and ta-da! You have your dataframe!\n",
    "final_df = pd.concat([new_names, new_no_names], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe, because you'll be\n",
    "# mad if you don't and you have to run all\n",
    "# this again\n",
    "final_df.to_csv('FlareTypeStars_Pandas_SimbadNames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
