{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import astropy.coordinates as coord\n",
    "import astropy.units as un\n",
    "from astroquery.simbad import Simbad\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_beam_size(freq, D):\n",
    "    '''\n",
    "    Calculate the primary beam size\n",
    "\n",
    "    Args:\n",
    "    freq (float): frequency in Hz\n",
    "    D (float): maximum baseline length\n",
    "               in m\n",
    "\n",
    "    Returns:\n",
    "    A float of the primary beam\n",
    "    diammeter in degrees\n",
    "    '''\n",
    "    # First convert the frequency\n",
    "    # to wavelength in m\n",
    "    lamda = 299792458/freq\n",
    "    # Then calculate the primary\n",
    "    # beam diammeter\n",
    "    return np.rad2deg((1.22*lamda)/D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find known flare objects in the FoV of MeerKAT LSP pointings\n",
    "\n",
    "This code can be used to find which known flare type objects are in the field of view (FoV) of MeerKAT LSP pointings. It has three parts:\n",
    "\n",
    "- Combine flare type object catalogues\n",
    "- Combine MeerKAT LSP pointing information\n",
    "- Find flare type objects that fall into the LSP fields of view\n",
    "\n",
    "The results of each step are saved as pandas data frames in csv files. This is to keep track of what's in all the columns.\n",
    "\n",
    "## Step 1: combine flare type object catalogues\n",
    "\n",
    "Here we read and collate catalogues of flare type objects. The catalogues we use include flare stars, white dwarfs, RS CVns, unidentified transient flares and more. The catalogues we use here are:\n",
    "\n",
    "- Lepine: bright M-dwarfs - 2011AJ....142..138L\n",
    "- JGagne: ultra-cool dwarfs - jgagneastro.wordpress.com/list-of-ultracool-dwarfs\n",
    "- sdss_mwds: magnetic white dwarfs from SDSS: 2013MNRAS.429.2934K\n",
    "- Catalina: Transients in the Catalina Surveys Data Release 2 - 2009ApJ...696..870D\n",
    "- simbad_fstars: flare stars (F*) from the SIMBAD catalogue (2000A&AS..143....9W)\n",
    "- simbad_RSCVns: flare stars (RS*) from the SIMBAD catalogue (2000A&AS..143....9W)\n",
    "- whitedwarfs: white dwarfs from the Montreal White Dwarf Database - http://www.montrealwhitedwarfdatabase.org/references.html\n",
    "\n",
    "We read in each catalogue using pandas, make columns for the proper motion if there are none and rename the RA and DEC columns. Then we combine the catalogues. If the source does not have an official SIMBAD name already, we check SIMBAD by radius to see if the source is known. Please note that this means that the SIMBAD names are *not* confirmed, but are a good starting point for looking into the source. Also, not all sources are identified in SIMBAD, particularly Catalina transients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files for the catalogues\n",
    "\n",
    "# Path to catalogue files\n",
    "path = '/raid/driessen/Catalogues/'\n",
    "\n",
    "# The catalogue files, downloaded from the\n",
    "# above sources\n",
    "lepine = ('{}Lepine_BrightMDwarfs.csv'.format(path))\n",
    "jgagne = ('{}List_of_UltraCool_Dwarfs.csv'.format(path))\n",
    "sdss_mwds = ('{}magWDs.tsv'.format(path))\n",
    "simbad_fstars = ('{}simbad_flarestars_pm.txt'.format(path))\n",
    "catalina = ('{}CRTS_all_transients.tsv'.format(path))\n",
    "catalinas = glob.glob('{}css_transientcandidates_?.tsv'.format(path))\n",
    "whitedwarfs = ('{}MWDD-export.csv'.format(path))\n",
    "simbad_RSCVns = ('{}SIMBAD_RSCVns.txt'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files\n",
    "\n",
    "Read in the files usin pandas. Each one has to be read in slightly differently because the catalogues are formatted very differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lepine_source_info = []\n",
    "columns = ['lepine_name', 'cns3_name',\n",
    "           'ra(deg)', 'dec(deg)',\n",
    "           'pmra(arcsec/yr)', 'pmdec(arcsec/yr)']\n",
    "\n",
    "# Lepine uses an odd file format that Pandas\n",
    "# doesn't like, so read it in line by line\n",
    "with open(lepine, 'r') as fn:\n",
    "    for l, line in enumerate(fn):\n",
    "        if l > 40:\n",
    "            lepinename = line[:17].strip()\n",
    "            cnsname = line[38:55].strip()\n",
    "            ra = line[56:66].strip()\n",
    "            dec = line[68:78].strip()\n",
    "            pmra = line[86:92].strip()\n",
    "            pmdec = line[93:99].strip()\n",
    "            \n",
    "            lepine_source_info.append([lepinename, cnsname,\n",
    "                                       ra, dec, pmra, pmdec])  \n",
    "lepine_source_info = np.array(lepine_source_info)\n",
    "\n",
    "# Put everything into a Pandas table\n",
    "lepine_dict = dict()\n",
    "for c, col in enumerate(columns):\n",
    "    lepine_dict[col] = lepine_source_info[:, c]\n",
    "lepine_table = pd.DataFrame(data=lepine_dict)\n",
    "\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "lepine_table = lepine_table.dropna(axis='rows', subset=['ra(deg)',\n",
    "                                                        'dec(deg)'])\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "lepine_table['Catalogue'] = 'Lepine'\n",
    "# Add an empty column for SIMBAD names\n",
    "lepine_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jgagne_table = pd.read_csv(jgagne)\n",
    "# Change the column names so that every\n",
    "# catalogue has the same name\n",
    "# for the RA and Dec columns\n",
    "jgagne_table = jgagne_table.rename(columns={'R.A. (deg)':'ra(deg)',\n",
    "                                            'Decl. (deg)':'dec(deg)'})\n",
    "jgagne_table = jgagne_table.dropna(axis='rows', subset=['ra(deg)',\n",
    "                                                        'dec(deg)'])\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "jgagne_table['Catalogue'] = 'J.Gagne'\n",
    "# Add an empty column for SIMBAD names\n",
    "# and proper motions\n",
    "jgagne_table['pmra(arcsec/yr)'] = ''\n",
    "jgagne_table['pmdec(arcsec/yr)'] = ''\n",
    "jgagne_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vizier_mwds_table = pd.read_csv(sdss_mwds, header=[71, 72], delimiter='\\t')\n",
    "vizier_mwds_table = vizier_mwds_table[1:]\n",
    "vizier_mwds_table.columns = vizier_mwds_table.columns.map('_'.join)\n",
    "\n",
    "# Change the column names so that every\n",
    "# catalogue has the same name\n",
    "# for the RA and Dec columns\n",
    "vizier_mwds_table = vizier_mwds_table.rename(columns={'#_RAJ2000_#deg':'ra(deg)',\n",
    "                                                      '_DEJ2000_deg':'dec(deg)'})\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "vizier_mwds_table = vizier_mwds_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "vizier_mwds_table['Catalogue'] = 'Vizier_MWDs'\n",
    "# Add an empty column for SIMBAD names\n",
    "# and proper motions\n",
    "vizier_mwds_table['pmra(arcsec/yr)'] = ''\n",
    "vizier_mwds_table['pmdec(arcsec/yr)'] = ''\n",
    "vizier_mwds_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbad_fstars_table = pd.read_csv(simbad_fstars, header=[4], delimiter='|')\n",
    "simbad_fstars_table = simbad_fstars_table[1:-1]\n",
    "simbad_fstars_table.columns = simbad_fstars_table.columns.str.strip()\n",
    "\n",
    "# This catalogue has an odd coordinate\n",
    "# format, so I first correct that\n",
    "simbad_coords = []\n",
    "simbad_coords_orig = simbad_fstars_table['coord1 (ICRS,J2000/2000)']\n",
    "\n",
    "for c, coo in enumerate(simbad_coords_orig):\n",
    "    if '+' in coo:\n",
    "        coordi = coo.split(' +')\n",
    "        coordi = [coordi[0], '+'+coordi[1]]\n",
    "    elif '-' in coo:\n",
    "        coordi = coo.split(' -')\n",
    "        coordi = [coordi[0], '-'+coordi[1]]\n",
    "    simbad_coords.append(coordi)\n",
    "simbad_coords = coord.SkyCoord(simbad_coords, unit=(un.hourangle, un.deg))\n",
    "# Add the corrected RA and DEC columns\n",
    "simbad_fstars_table['ra(deg)'] = np.array(simbad_coords.ra.deg)\n",
    "simbad_fstars_table['dec(deg)'] = np.array(simbad_coords.dec.deg)\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "simbad_fstars_table = simbad_fstars_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "\n",
    "# Get the proper motions\n",
    "# and put them into the correct\n",
    "# format\n",
    "pmra = []\n",
    "pmdec = []\n",
    "for pm in simbad_fstars_table['pm']:\n",
    "    if '~' in pm:\n",
    "        pmra.append(np.nan)\n",
    "        pmdec.append(np.nan)\n",
    "    else:\n",
    "        pms = pm.split()\n",
    "        pmra.append(pms[0])\n",
    "        pmdec.append(pms[1])\n",
    "simbad_fstars_table['pmra(arcsec/yr)'] = pmra\n",
    "simbad_fstars_table['pmdec(arcsec/yr)'] = pmdec\n",
    "\n",
    "# Change the column name that has\n",
    "# the SIMBAD names in it, for\n",
    "# consistancy\n",
    "simbad_fstars_table = simbad_fstars_table.rename(columns={'identifier':\n",
    "                                                          'simbad_names'})\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "simbad_fstars_table['Catalogue'] = 'SimbadFlareStars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbad_rscvns_table = pd.read_csv(simbad_RSCVns, header=[4], delimiter='|')\n",
    "simbad_rscvns_table = simbad_rscvns_table[1:-1]\n",
    "simbad_rscvns_table.columns = simbad_rscvns_table.columns.str.strip()\n",
    "\n",
    "# This catalogue has an odd coordinate\n",
    "# format, so I first correct that\n",
    "simbad_coords = []\n",
    "simbad_coords_orig = simbad_rscvns_table['coord1 (ICRS,J2000/2000)']\n",
    "\n",
    "for c, coo in enumerate(simbad_coords_orig):\n",
    "    if '+' in coo:\n",
    "        coordi = coo.split(' +')\n",
    "        coordi = [coordi[0], '+'+coordi[1]]\n",
    "    elif '-' in coo:\n",
    "        coordi = coo.split(' -')\n",
    "        coordi = [coordi[0], '-'+coordi[1]]\n",
    "    simbad_coords.append(coordi)\n",
    "simbad_coords = coord.SkyCoord(simbad_coords, unit=(un.hourangle,\n",
    "                                                    un.deg))\n",
    "# Add the corrected RA and DEC columns\n",
    "simbad_rscvns_table['ra(deg)'] = np.array(simbad_coords.ra.deg)\n",
    "simbad_rscvns_table['dec(deg)'] = np.array(simbad_coords.dec.deg)\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "simbad_rscvns_table = simbad_rscvns_table.dropna(axis='rows',\n",
    "                                                 subset=['ra(deg)',\n",
    "                                                         'dec(deg)'])\n",
    "\n",
    "# Get the proper motions\n",
    "# and put them into the correct\n",
    "# format\n",
    "pmra = []\n",
    "pmdec = []\n",
    "for pm in simbad_rscvns_table['pm']:\n",
    "    if '~' in pm:\n",
    "        pmra.append(np.nan)\n",
    "        pmdec.append(np.nan)\n",
    "    else:\n",
    "        pms = pm.split()\n",
    "        pmra.append(pms[0])\n",
    "        pmdec.append(pms[1])\n",
    "simbad_rscvns_table['pmra(arcsec/yr)'] = pmra\n",
    "simbad_rscvns_table['pmdec(arcsec/yr)'] = pmdec\n",
    "# Change the column name that has\n",
    "# the SIMBAD names in it, for\n",
    "# consistancy\n",
    "simbad_rscvns_table = simbad_rscvns_table.rename(columns={'identifier':\n",
    "                                                          'simbad_names'})\n",
    "\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "simbad_rscvns_table['Catalogue'] = 'SimbadRSCVns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the Catalina sources is a bit more\n",
    "# complicated because the sources are in\n",
    "# a set of files, rather than one file\n",
    "\n",
    "headers = ['CRTS_ID', 'ra(deg)', 'dec(deg)', 'classification']\n",
    "new_content = []\n",
    "with open(catalina, 'r') as content:\n",
    "    for l, line in enumerate(content):\n",
    "        if l>0:\n",
    "            line = line.split('\\t')\n",
    "            if len(line) >= 10:\n",
    "                nc = [line[0].strip(), line[1].strip(), line[2].strip(), line[-1].strip()]\n",
    "                new_content.append(nc)\n",
    "values = np.array(new_content, dtype=str)\n",
    "\n",
    "catalina_dict = dict()\n",
    "\n",
    "for h, head in enumerate(headers):\n",
    "    catalina_dict[head] = values[:, h]\n",
    "catalina_table = pd.DataFrame(data=catalina_dict)\n",
    "\n",
    "# Remove rows that don't have known\n",
    "# coordinates\n",
    "catalina_table = catalina_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "\n",
    "for cat in catalinas:\n",
    "    headers = ['CRTS_ID', 'ra(deg)', 'dec(deg)', 'classification']\n",
    "    new_content = []\n",
    "    with open(cat, 'r') as content:\n",
    "        for l, line in enumerate(content):\n",
    "            if l>0:\n",
    "                line = line.split('\\t')\n",
    "                if len(line) >= 10:\n",
    "                    nc = [line[0].strip(), line[1].strip(), line[2].strip(), line[-1].strip()]\n",
    "                    new_content.append(nc)\n",
    "    values = np.array(new_content, dtype=str)\n",
    "\n",
    "    cat_dict = dict()\n",
    "\n",
    "    for h, head in enumerate(headers):\n",
    "        cat_dict[head] = values[:, h]\n",
    "    cat_table = pd.DataFrame(data=cat_dict)\n",
    "    \n",
    "    # Remove rows that don't have known\n",
    "    # coordinates\n",
    "    cat_table = cat_table.dropna(axis='rows', subset=['ra(deg)', 'dec(deg)'])\n",
    "\n",
    "    catalina_table = catalina_table.append(cat_table)\n",
    "\n",
    "subs = ['SN', 'Ast', 'AGN', 'Nothing', 'Blazar']\n",
    "for sub in subs:\n",
    "    catalina_table['match_indexes'] = catalina_table['classification'].str.find(sub)\n",
    "    catalina_table = catalina_table[catalina_table['match_indexes'] == -1]\n",
    "    catalina_table.drop('match_indexes', axis='columns', inplace=True)\n",
    "\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "catalina_table['Catalogue'] = 'CRTS'\n",
    "# Add an empty column for SIMBAD names\n",
    "# and proper motions\n",
    "catalina_table['pmra(arcsec/yr)'] = ''\n",
    "catalina_table['pmdec(arcsec/yr)'] = ''\n",
    "catalina_table['simbad_names'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_table = pd.read_csv(whitedwarfs)\n",
    "\n",
    "# Put the coordinates in the right\n",
    "# format then add them as columns\n",
    "wd_coords = coord.SkyCoord(wd_table['icrsra'],\n",
    "                           wd_table['icrsdec'],\n",
    "                           unit=(un.hourangle, un.deg))\n",
    "wd_table['ra(deg)'] = wd_coords.ra.deg\n",
    "wd_table['dec(deg)'] = wd_coords.dec.deg\n",
    "# Change the name column for consistency\n",
    "wd_table = wd_table.rename(columns={'wdid':'simbad_names'})\n",
    "# Make a column stating which catalogue these\n",
    "# sources are from\n",
    "wd_table['Catalogue'] = 'MWDD'\n",
    "# Make empty columns for the\n",
    "# proper motions\n",
    "wd_table['pmra(arcsec/yr)'] = ''\n",
    "wd_table['pmdec(arcsec/yr)'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the dataframes\n",
    "\n",
    "Combine the pandas dataframes for each catalogue. Use \"inner\" so that only identical columns are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [lepine_table,\n",
    "              jgagne_table,\n",
    "              vizier_mwds_table,\n",
    "              simbad_fstars_table,\n",
    "              catalina_table,\n",
    "              wd_table,\n",
    "              simbad_rscvns_table]\n",
    "\n",
    "combined_df = pd.concat(dataframes, join='inner', ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "combined_df[['ra(deg)',\n",
    "             'dec(deg)',\n",
    "             'pmra(arcsec/yr)',\n",
    "             'pmdec(arcsec/yr)']] = combined_df[['ra(deg)',\n",
    "                                                 'dec(deg)',\n",
    "                                                 'pmra(arcsec/yr)',\n",
    "                                                 'pmdec(arcsec/yr)']].apply(pd.to_numeric,\n",
    "                                                                            downcast='float')\n",
    "# Split the catalogue into sources that already\n",
    "# have SIMBAD names, and sources that don't\n",
    "no_names = combined_df[combined_df['simbad_names'] == '']\n",
    "names = combined_df[combined_df['simbad_names'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find SIMBAD names for sources that don't have them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coordinates of the sources\n",
    "# that don't have names yet\n",
    "no_name_coords = coord.SkyCoord(np.array(no_names['ra(deg)'])*un.deg,\n",
    "                                np.array(no_names['dec(deg)'])*un.deg,\n",
    "                                pm_ra_cosdec=np.array(no_names['pmra(arcsec/yr)'])*un.arcsec/un.yr,\n",
    "                                pm_dec=np.array(no_names['pmdec(arcsec/yr)'])*un.arcsec/un.yr)\n",
    "no_name_names = []\n",
    "\n",
    "# Divide the sources up into chunks,\n",
    "# otherwise astroquery will chuck\n",
    "# a hissy fit\n",
    "for s, starts in enumerate(np.arange(0, 16)):\n",
    "    start = starts * 1000\n",
    "    end = start + 1000\n",
    "    coords = no_name_coords[start:end]\n",
    "\n",
    "    # Use the coordinates to search for nearby\n",
    "    # sources for each source\n",
    "    result_table = Simbad.query_region(coords, radius=2.*un.arcsec)\n",
    "    # Match the results to the sources\n",
    "    try:\n",
    "        result_coords = coord.SkyCoord(list(result_table['RA']),\n",
    "                                       list(result_table['DEC']),\n",
    "                                       unit=(un.hourangle, un.deg))\n",
    "\n",
    "        for c, coo in enumerate(coords):\n",
    "            seps = coo.separation(result_coords)\n",
    "\n",
    "            if np.nanmin(seps.deg) < 2./60./60.:\n",
    "                no_name_names.append(result_table[np.nanargmin(seps.deg)]['MAIN_ID'].decode('UTF-8'))\n",
    "            else:\n",
    "                no_name_names.append('')\n",
    "    except (TypeError, KeyError) as e:\n",
    "        # Add a space for any source that doesn't\n",
    "        # have any SIMBAD matches\n",
    "        print('No SIMBAD matches within 2 asec: ', start, end)\n",
    "        for c, coo in enumerate(coords):\n",
    "            no_name_names.append('')\n",
    "    # Take a break, because otherwise astroquery\n",
    "    # and SIMBAD will have a different hissy fit\n",
    "    time.sleep(10)\n",
    "\n",
    "# Add the SIMBAD names you just found\n",
    "# to the source without names\n",
    "no_names['simbad_names'] = no_name_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the tables again\n",
    "combined_df_names = pd.concat([names, no_names], join='inner', ignore_index=True)\n",
    "# Split them into sources with and without names again\n",
    "new_no_names = combined_df_names[combined_df_names['simbad_names'] == '']\n",
    "new_names = combined_df_names[combined_df_names['simbad_names'] != '']\n",
    "# Remove sources that have the same name\n",
    "new_names = new_names.drop_duplicates(subset='simbad_names')\n",
    "# Combine them again, and ta-da! You have your dataframe!\n",
    "final_df = pd.concat([new_names, new_no_names], join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe, because you'll be\n",
    "# mad if you don't and you have to run all\n",
    "# this again\n",
    "final_df.to_csv('{}FlareTypeStars_Pandas_SimbadNames.csv'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: combine MeerKAT LSP pointing information\n",
    "\n",
    "Here we read in and reorganise the information about the MeerKAT LSP pointings. We include the RA and DEC of the phase centre of each pointing, the diameter of the primary beam, the source at the phase centre (if there is one), and the name of the LSP.\n",
    "\n",
    "Start by reading in the information for the LSPs: MALS, ThunderKAT, Fornax, LADUMA, MHONGOOSE and MIGHTEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALS_raw = np.genfromtxt('{}MALS-Lband-sample.txt'.format(path))\n",
    "\n",
    "# Get the RA and DEC from the array\n",
    "ras = MALS_raw[:, :3]\n",
    "decs = MALS_raw[:, 3:6]\n",
    "# Convert the RA and DEC into the\n",
    "# required format for astropy SkyCoord\n",
    "mals_ra = []\n",
    "mals_dec = []\n",
    "for r, ra in enumerate(ras):\n",
    "    mals_ra.append('{0}h{1}m{2}s'.format(int(ra[0]),\n",
    "                               int(ra[1]),\n",
    "                               ra[2]))\n",
    "    mals_dec.append('{0}d{1}m{2}s'.format(int(decs[r][0]),\n",
    "                                int(decs[r][1]),\n",
    "                                decs[r][2]))\n",
    "radec = coord.SkyCoord(mals_ra, mals_dec, frame='icrs')\n",
    "freq = MALS_raw[:, 7]*1e9\n",
    "pb = get_primary_beam_size(freq, 13.5)\n",
    "\n",
    "mals_dict = {'RA(deg)': radec.ra.deg,\n",
    "             'DEC(deg)': radec.dec.deg,\n",
    "             'Freq(Hz)':freq,\n",
    "             'Beam_FWHM_Diameter(deg)': pb}\n",
    "mals_df = pd.DataFrame(data=mals_dict)\n",
    "mals_df['LSP'] = 'MALS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laduma_df = pd.read_csv('{}LADUMA_coordinates.csv'.format(path))\n",
    "laduma_df['Freq(Hz)'] = 1.4e9\n",
    "laduma_df['LSP'] = 'LADUMA'\n",
    "laduma_df['Beam_FWHM_Diameter(deg)'] = get_primary_beam_size(1.4e9,\n",
    "                                                             13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkt_df = pd.read_csv('{}ThunderKAT_coordinates.csv'.format(path))\n",
    "tkt_df['Freq(Hz)'] = 1.4e9\n",
    "tkt_df['LSP'] = 'ThunderKAT'\n",
    "tkt_df['Beam_FWHM_Diameter(deg)'] = get_primary_beam_size(1.4e9,\n",
    "                                                          13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhon_raw = np.genfromtxt('{}mhongoose_candidates.txt'.format(path),\n",
    "                         dtype=str)\n",
    "mhon_coords = coord.SkyCoord(mhon_raw[:, 3], mhon_raw[:, 4], frame='icrs')\n",
    "\n",
    "mhon_dict = {'RA(deg)': mhon_coords.ra.deg,\n",
    "             'DEC(deg)': mhon_coords.dec.deg}\n",
    "mhon_df = pd.DataFrame(data=mhon_dict)\n",
    "mhon_df['Freq(Hz)'] = 1.4e9\n",
    "mhon_df['LSP'] = 'MHONGOOSE'\n",
    "mhon_df['Beam_FWHM_Diameter(deg)'] = get_primary_beam_size(1.4e9,\n",
    "                                                           13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fornax_raw = np.genfromtxt(('{}Fornax_mfs_pointingcentres_'\n",
    "                            '2018_radec.txt').format(path))\n",
    "fornax_coords = coord.SkyCoord(fornax_raw,\n",
    "                               unit=(un.deg, un.deg))\n",
    "fornax_dict = {'RA(deg)': fornax_coords.ra.deg,\n",
    "               'DEC(deg)': fornax_coords.dec.deg}\n",
    "fornax_df = pd.DataFrame(data=fornax_dict)\n",
    "fornax_df['Freq(Hz)'] = 1.4e9\n",
    "fornax_df['LSP'] = 'Fornax'\n",
    "fornax_df['Beam_FWHM_Diameter(deg)'] = get_primary_beam_size(1.4e9,\n",
    "                                                             13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mightee_raw = np.genfromtxt('{}MIGHTEE_pointings.tsv'.format(path),\n",
    "                            dtype=str)\n",
    "mightee_info = []\n",
    "for row in mightee_raw:\n",
    "    if row[0] == 'COSMOS':\n",
    "        mightee_info.append([float(row[1]), float(row[2]), 1.4e9])\n",
    "    elif 'CDFS' in row[0]:\n",
    "        mightee_info.append([float(row[1]), float(row[2]), 1.4e9])\n",
    "        mightee_info.append([float(row[1]), float(row[2]), 0.85e9])\n",
    "        mightee_info.append([float(row[1]), float(row[2]), 2.125e9])\n",
    "    else:\n",
    "        mightee_info.append([float(row[1]), float(row[2]), 1.4e9])\n",
    "mightee_info = np.array(mightee_info)\n",
    "\n",
    "mightee_coords = coord.SkyCoord(mightee_info[:, 0].astype(float)*un.deg,\n",
    "                                mightee_info[:, 1].astype(float)*un.deg)\n",
    "\n",
    "freq = mightee_info[:, 2]\n",
    "pb = get_primary_beam_size(freq,\n",
    "                           13.5)\n",
    "\n",
    "mightee_dict = {'RA(deg)': mightee_coords.ra.deg,\n",
    "                'DEC(deg)': mightee_coords.dec.deg,\n",
    "                'Freq(Hz)':freq,\n",
    "                'Beam_FWHM_Diameter(deg)': pb}\n",
    "mightee_df = pd.DataFrame(data=mightee_dict)\n",
    "mightee_df['LSP'] = 'MIGHTEE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all of the known pulsars as\n",
    "# a proxy for MeerTIME pointing positions\n",
    "meerTime_raw = np.genfromtxt('{}formatted_ATNF_psrs.csv'.format(path),\n",
    "                             delimiter=',', dtype=str)\n",
    "\n",
    "meertime_coords = coord.SkyCoord(meerTime_raw[:, 0].astype(float)*un.deg,\n",
    "                                 meerTime_raw[:, 1].astype(float)*un.deg)\n",
    "\n",
    "meertime_dict = {'RA(deg)': meertime_coords.ra.deg,\n",
    "                 'DEC(deg)': meertime_coords.dec.deg}\n",
    "meertime_df = pd.DataFrame(data=meertime_dict)\n",
    "meertime_df['Freq(Hz)'] = 1.4e9\n",
    "meertime_df['LSP'] = 'pulsars'\n",
    "meertime_df['Beam_FWHM_Diameter(deg)'] = get_primary_beam_size(1.4e9,\n",
    "                                                               13.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data frames for all of the LSPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [mals_df, laduma_df, tkt_df, mhon_df, fornax_df, meertime_df, mightee_df]\n",
    "\n",
    "combined_df = pd.concat(dataframes, join='inner', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the source at the phase centre of each pointing (if there is one). This is just some nice extra information to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coords = coord.SkyCoord(combined_df['RA(deg)']*un.deg,\n",
    "                            combined_df['DEC(deg)']*un.deg)\n",
    "centre_source = []\n",
    "\n",
    "# Divide the sources up into chunks,\n",
    "# otherwise astroquery will chuck\n",
    "# a hissy fit\n",
    "for s, starts in enumerate(np.arange(0, 4)):\n",
    "    start = starts * 1000\n",
    "    end = start + 1000\n",
    "    coords = all_coords[start:end]\n",
    "\n",
    "    # Use the coordinates to search for nearby\n",
    "    # sources for each source\n",
    "    result_table = Simbad.query_region(coords, radius=2.*un.arcsec)\n",
    "    # Match the results to the sources\n",
    "    try:\n",
    "        result_coords = coord.SkyCoord(list(result_table['RA']),\n",
    "                                       list(result_table['DEC']),\n",
    "                                       unit=(un.hourangle, un.deg))\n",
    "\n",
    "        for c, coo in enumerate(coords):\n",
    "            seps = coo.separation(result_coords)\n",
    "\n",
    "            if np.nanmin(seps.deg) < 2./60./60.:\n",
    "                centre_source.append(result_table[np.nanargmin(seps.deg)]['MAIN_ID'].decode('UTF-8'))\n",
    "            else:\n",
    "                centre_source.append('')\n",
    "    except (TypeError, KeyError) as e:\n",
    "        # Add a space for any source that doesn't\n",
    "        # have any SIMBAD matches\n",
    "        print('No SIMBAD matches within 2 asec: ', start, end)\n",
    "        for c, coo in enumerate(coords):\n",
    "            centre_source.append('')\n",
    "combined_df['PhaseCentreSource'] = centre_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('{}MeerKAT_LSP_AllPointings.csv'.format(path),\n",
    "                   index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: find flare type objects that fall into the LSP fields of view\n",
    "\n",
    "Check whether any flare type objects are in the field of view for MeerKAT LSPs.\n",
    "\n",
    "In the final data frame, there is a row for every flare star that is within the primary beam of an LSP. That means that some flare stars are counted twice, if an LSP has overlapping beams and the flare star falls in both. You can easily remove stars that are counted twice by using drop_duplicates, I'll demonstrate this near the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the flare objects file\n",
    "flarestars = pd.read_csv(('{}FlareTypeStars_'\n",
    "                          'Pandas_'\n",
    "                          'SimbadNames.csv').format(path)).drop(['Unnamed: 0'],\n",
    "                                                                axis=1)\n",
    "# Get the coordinates of each source\n",
    "fstar_coords = SkyCoord(flarestars['ra(deg)'].astype(float)*un.degree,\n",
    "                        flarestars['dec(deg)'].astype(float)*un.degree,\n",
    "                        pm_ra_cosdec=(np.array(flarestars['pmra(arcsec/yr)']).astype(float)*un.mas/un.yr),\n",
    "                        pm_dec=(np.array(flarestars['pmdec(arcsec/yr)']).astype(float)*un.mas/un.yr),\n",
    "                        obstime=Time('2019-01-01T00:00:00.00'))\n",
    "\n",
    "# Read in the LSP catalogue\n",
    "lsps = pd.read_csv('MeerKAT_LSP_AllPointings.csv')\n",
    "# Remove MeerTIME, since these coordinates\n",
    "# are educated guesses, and the integration\n",
    "# time per pointing is very short\n",
    "lsps_nomeertime = lsps[lsps['LSP'] != 'pulsars']\n",
    "# Get the LSP coordinates\n",
    "lsp_coords = SkyCoord(lsps_nomeertime['RA(deg)'].astype(float)*un.degree,\n",
    "                      lsps_nomeertime['DEC(deg)'].astype(float)*un.degree,\n",
    "                      obstime=Time('2019-01-01T00:00:00.00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the columns for your final\n",
    "# data frame by combining the flare star\n",
    "# (FSTAR) and LSP (LSP) file\n",
    "# columns\n",
    "cols = []\n",
    "for col in flarestars.columns:\n",
    "    cols.append('FSTAR '+col)\n",
    "for col in lsps.columns:\n",
    "    if '#' in col:\n",
    "        cols.append('LSP index')\n",
    "    elif 'LSP' in col:\n",
    "        cols.append('LSP')\n",
    "    else:\n",
    "        cols.append('LSP '+col.strip())\n",
    "cols.append('min_sep(deg)')\n",
    "\n",
    "matched_sources = []\n",
    "# For each LSP pointing, check which\n",
    "# stars are within the primary beam\n",
    "# at the centre of the observing band\n",
    "for index, row in lsps_nomeertime.iterrows():\n",
    "    lsp_coord = SkyCoord(row['RA(deg)']*un.degree,\n",
    "                         row['DEC(deg)']*un.degree,\n",
    "                         obstime=Time('2019-01-01T00:00:00.00'))\n",
    "    beam_size = float(row['Beam_FWHM_Diameter(deg)'])\n",
    "    seps = lsp_coord.separation(fstar_coords)\n",
    "    in_beam = np.where(seps.deg<0.5*beam_size)[0]\n",
    "\n",
    "    if len(in_beam)> 0:\n",
    "        for b in in_beam:\n",
    "            source_info = []\n",
    "            for col in flarestars.columns:\n",
    "                fstar_b = flarestars.iloc[b]\n",
    "                source_info.append(str(fstar_b[col]))\n",
    "            for col in lsps.columns:\n",
    "                source_info.append(str(row[col]))\n",
    "            source_info.append(str(seps.deg[b]))\n",
    "            matched_sources.append(source_info)\n",
    "matched_sources = np.array(matched_sources)\n",
    "\n",
    "# Take your matched sources and put\n",
    "# them in a data frame\n",
    "new_table = dict()\n",
    "for c, col in enumerate(cols):\n",
    "    new_table[col] = matched_sources[:, c]\n",
    "fstar_lsps = pd.DataFrame(data=new_table)\n",
    "\n",
    "# Save that data frame\n",
    "fstar_lsps.to_csv('{}FStar_LSP_Matches.csv'.format(path), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fstar_lsps is the final data frame with all of the matches. But maybe you want just the unique stars that are in at least one LSP. Then you can use drop_duplicates to get rid of copies of the same star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_fstars = fstar_lsps.drop_duplicates(subset=['FSTAR ra(deg)', 'FSTAR dec(deg)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can, for example, have a look at just the SIMBAD RS CVns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_fstars[unique_fstars['FSTAR Catalogue']=='SimbadRSCVns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you could have a look at all the matches for a single LSP, such as THUNDERKAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_fstars[unique_fstars['LSP']=='ThunderKAT']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
